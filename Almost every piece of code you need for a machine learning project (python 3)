# -*- coding: utf-8 -*-

#Importing Libraries

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split   #
from sklearn.tree import DecisionTreeRegressor         #loading descision tree
from sklearn.ensemble import RandomForestRegressor     #loading random forest 
from sklearn.preprocessing import OrdinalEncoder       #loading ordinal encoder
from sklearn.preprocessing import OneHotEncoder        loading one hote encoder

#----------------------------Section 1 - Exploratory Data Analysis----------------------------

# READ THE CSV FILE

#-- Read the training file
df = pd.read_csv('C:\\Users\\syed1\\Documents\\Learning\\Hackathons\\Phishing Detection\\Phising_Training_Dataset\\Phising_Training_Dataset.csv')

#-- Read the test file 
df_test = pd.read_csv('C:\\Users\\syed1\\Documents\\Learning\\Hackathons\\Phishing Detection\\Phising_Testing_Dataset\\Phising_Testing_Dataset.csv')

# INSPECT THE FILE

# head() prints first few rows
print(df.head())

# info() shows information on each col. Like data types, missing etc
print(df.info())

# shape() gives the number of rows and columns
print(df.shape)

# Print column names only
print(sorted(df))

# Find out the type of variable
type(df)

# Selecting a column 
df['has_an_application']

# Select distinct values from a column
df['level_of_study_new'].drop_duplicates()

# Select too 5 unique values 
df['has_an_application'].drop_duplicates().head(5)

# Where Clause
df[df['level_of_study_new'] == 'graduate'].head(5)

# Where Clause with AND and OR


# Max Correlation of one column related to another
abs_corrmat = np.abs(df.corr())
max_corr = abs_corrmat.apply(lambda x: sorted(x)[-2])
print('Maximum Correlation possible for each column: ', np.round(max_corr.tolist(), 2))


# MISSING VALUES

# Check for missing values in the dataframe
df.isnull().values.any()

# Count for missing values in each column of the dataframe
missing_val_count_by_column = df.isna().sum()
print(missing_val_count_by_column[missing_val_count_by_column > 0])


# Plot a bar chart to visualize missing values in each column 
df.isnull().sum().reset_index(name = "count of missing").plot.bar(x='index',y='n',rot= 45)

df.isna().sum()[df.isna().sum()>0].plot(kind='bar')


# ALTER DATAFRAME

#Converting all the columns in a dataframe into a numeric. errors=ignore will convert only the columns that are eligible
df1 = df.apply(pd.to_numeric, errors='ignore') # converting to numeric
print(df1.dtypes)  # checking data types
df1.select_dtypes([np.number]).columns # checking the numeric columns only. We now have 21 columns as truly numeric 

# Creating a subset of an existing data frame based on the column data types
numeric_columns = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_num = df1.select_dtypes(include=numeric_columns)

# Sort Columns in Ascending/Descending Order
df.sort_index(axis=1, ascending=False, inplace=True)  # Use Ascending = T/F to make specify your sorting

# Join Two data frames on multiple columns
pd.merge(df1, df2, how='inner', left_on=['fruit', 'weight'], right_on=['pazham', 'pounds'], suffixes=['_left', '_right'])

# Create One hot ecoding of a categorical variable
df = pd.DataFrame(np.arange(25).reshape(5,-1), columns=list('DNSRecord'))



# ADDRESSING THE CATGEORICAL VARIABLES

# Get list of categorical variables
s = (X_train.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables:")
print(object_cols)

# Apply "ordinal encoder" to each column with categorical data
ordinal_encoder = OrdinalEncoder()
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])


# Apply "one-hot encoder" to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)



#----------------------------Section 2 - Train Test Split--------------------------------------------


# (1) Create target object and call it y
y = df.Result

# (2) Create a features List 
features = ['Abnormal_URL', 'DNSRecord', 'Domain_registeration_length', 'Favicon', 'Google_Index', 'HTTPS_token', 'Iframe', 'Links_in_tags', 'Links_pointing_to_page', 'Page_Rank', 'Prefix_Suffix', 'Redirect', 'Request_URL', 'RightClick', 'SFH', 'SSLfinal_State', 'Shortining_Service', 'Statistical_report', 'Submitting_to_email', 'URL_Length', 'URL_of_Anchor', 'age_of_domain', 'double_slash_redirecting', 'having_At_Symbol', 'having_IP', 'having_Sub_Domain', 'key', 'on_mouseover', 'popUpWidnow', 'port', 'web_traffic']

# (3) Create feature Object X to be used for prediction
X = df[features]
X_test = df_test[features].copy()

# (4) Split into validation and training data
train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 0)

print(val_X.shape)
print(train_y.shape)



#----------------------------Section 3 - Model Building--------------------------------------------

# Building a single Decision Tree Model, fitting on test 

dt_model = DecisionTreeRegressor(random_state=1)
dt_model.fit(train_X, train_y)
val_preds = dt_model.predict(val_X)
print(mean_absolute_error(val_y, val_preds))


# Building a Random Forest Model, fitting on test 

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))

###  Model Pipeline for random Forest ####
# Define the models
model_1 = RandomForestRegressor(n_estimators=50, random_state=0)
model_2 = RandomForestRegressor(n_estimators=100, random_state=0)
model_3 = RandomForestRegressor(n_estimators=100, criterion='mse', random_state=0)
model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)
model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)

models = [model_1, model_2, model_3, model_4, model_5]

# Function for comparing different models
def score_model(model, X_t=train_X, X_v=val_X, y_t=train_y, y_v=val_y):
    model.fit(X_t, y_t)
    preds = model.predict(X_v)
    return mean_absolute_error(y_v, preds)

for i in range(0, len(models)):
    mae = score_model(models[i])
    print("Model %d MAE: %d" % (i+1, mae))
    
print(models)

my_model = model_2 # select the best model based your evaluation metrics 

# Fit the model to the training data
my_model.fit(X, y)

# Generate test predictions
preds_test = my_model.predict(X_test)

# Save predictions in format used for competition scoring  note: this is for competitions only 
output = pd.DataFrame({'key': X_test.key,
                       'Result': preds_test})
print(output.head())
output.to_csv('submission.csv', index=False)









